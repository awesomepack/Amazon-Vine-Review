{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.0.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to PostgreSQL\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Video Game reviews from S3 bucket\n",
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz\"\n",
    "spark.sparkContext.addFile(url)\n",
    "video_games_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Watches_v1_00.tsv.gz\"), sep=\"\\t\", header=True, inferSchema=True )\n",
    "\n",
    "# Show DataFrame\n",
    "video_games_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in video_games_df\n",
    "record_count = video_games_df.count()\n",
    "\n",
    "print(f'There are {record_count} records in this dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform video_games_df into the review_id_table schema in resources\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Create a list of the columns we want\n",
    "review_id_columns = ['review_id' , 'customer_id' , 'product_id' , 'product_parent' , 'review_date']\n",
    "\n",
    "# Subset video_games_df using the list\n",
    "review_id_df = video_games_df[review_id_columns]\n",
    "\n",
    "review_id_df.show()\n",
    "\n",
    "# Check that the dtypes match what is requested in the schema\n",
    "# review_date is string , that needs to change\n",
    "\n",
    "# Convert review_date to date type\n",
    "review_id_df = review_id_df.withColumn('review_date' , review_id_df['review_date'].cast(DateType()))\n",
    "\n",
    "# confirm type conversion\n",
    "review_id_df.dtypes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://mypostgresdb.c0iyntsfllgk.us-west-1.rds.amazonaws.com:5432/HW22\"\n",
    "config = {\"user\":\"root\", \n",
    "          \"password\": \"Postgres1\", \n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to review_id_table in PGadmin\n",
    "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating df to match products schema\n",
    "\n",
    "# Creating a list of the columns needed\n",
    "productList = ['product_id' , 'product_title']\n",
    "\n",
    "# Creating a subset from video_games_df\n",
    "products_df = video_games_df[[productList]]\n",
    "\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing products_df to the products schema\n",
    "review_id_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df to match the customers schema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Creating subset of video_games_df\n",
    "customers_df = video_games_df[customersList]\n",
    "\n",
    "\n",
    "# creating the customer count column\n",
    "customers_df = customers_df.withColumn('customer_count' ,lit(record_count) )\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing customers_df to the customers schema\n",
    "review_id_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df to match the vine schema\n",
    "\n",
    "# listing the needed columns\n",
    "vineColumns = ['review_id' , 'star_rating' , 'helpful_votes' , 'total_votes' , 'vine']\n",
    "\n",
    "# creating a subset of video_games_df\n",
    "vine_df = video_games_df[vineColumns]\n",
    "vine_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to the vine_table using vine_df\n",
    "review_id_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
